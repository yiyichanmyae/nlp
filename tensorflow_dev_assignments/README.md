## Labs and Assignments
C3 - Course 3 of the Specialization

W1 - Week 1 of the Course

## `Natural Language Processing in Tensorflow` of `DeepLearning.AI`
This is assignments collection of `Natural Language Processing in Tensorflow` Course from `DeepLearning.AI` on `Cousera`.

[C3W1_Assignment.ipynb](C3W21_Assignment.ipynb)
- The assignment file of Week 1 in `NLP in TF` with the `Solution`

[C3W2_Lab2_sarcasm_classifier.ipynb](C3W2_Lab2_sarcasm_classifier.ipynb) 
- includes how GlobalAveragePooling1D() works, 
  how hyperparameters like vocab size, padded sequences size, embedding dimensions can effect the confident of the predictions (based on loss),
  visualization of the results based on model's history 
  and how to check the embedding results from the embedding layers by using [Tensorflow Embedding Projector](https://projector.tensorflow.org/).
  
[C3W2_Lab3_imdb_subwords.ipynb](C3W2_Lab3_imdb_subwords.ipynb)
- Comparing `Tokenizer` and `SubwordTextEncoder`
- IMDB review classification and evaluation with pretrained Subword Tokenizer

[C3W2_Assignment.ipynb](C3W2_Assignment.ipynb)
- The assignment file of Week 2 in `NLP in TF`
- [BBC News Classification Dataset](https://www.kaggle.com/c/learn-ai-bbc/overview)

[C3W2_Assignment_withSolution.ipynb](C3W2_Assignment_withSolution.ipynb)
- need ot use [bbc-text.csv](bbc-text.csv)
- includes `preprocessing`, 'Tokenization`, 'Model building`, `Evaluation` with `visualization` and creating the file to test with [Tensorflow's Embedding Projector](https://projector.tensorflow.org/).
- uses the `Embedding`, `GlobalAveragePooling1D` and `Dense` layers only.
- compiles with `sparse_categorical_crossentropy` loss function and `adam` optimizer.

## `Natural Language Processing` Specialization of `DeepLearning.AI`

[Logistic Regression_W1_Assignment.ipynb](Logistic Regression_W1_Assignment.ipynb)
- assignment notebook without solution
- if you wanna check the solution, please see [LogisticRegression_fromScratch.ipynb](../LogisticRegression_fromScratch.ipynb)
